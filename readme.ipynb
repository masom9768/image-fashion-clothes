{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e389cc",
   "metadata": {},
   "source": [
    "# Design Fashion of Clothes with Generative Adversarial Networks\n",
    "\n",
    "\n",
    "\n",
    "![image](./docs/assets/clothes.png)\n",
    "**Figure:** *dress image editing controlled via style images and segmentation masks with SEAN*\n",
    "\n",
    "We propose semantic region-adaptive normalization (SEAN), a simple but effective building block for Generative Adversarial Networks conditioned on segmentation masks that describe the semantic regions in the desired output image. Using SEAN normalization, we can build a network architecture that can control the style of each semantic region individually, e.g., we can specify one style reference image per region. SEAN is better suited to encode, transfer, and synthesize style than the best previous method in terms of reconstruction quality, variability, and visual quality. We evaluate SEAN on datasets and report better quantitative metrics (e.g. FID, PSNR) than the current state of the art. SEAN also pushes the frontier of interactive image editing. We can interactively edit images by changing segmentation masks or the style for any given region. We can also interpolate styles from two reference images per region.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c14a3a",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "\n",
    "This code requires PyTorch, python 3+ and Pyqt5. Please install dependencies by\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "This model requires a lot of memory and time to train. To speed up the training, we recommend using multi GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21fcad",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "This code uses [deepfashion] (https://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) dataset. The prepared dataset can be directly downloaded [here] (https://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html). After unzipping, put the entire deepfashion clothes(dfc) folder in the datasets folder. The complete directory should look like `./datasets/train/` and `./datasets/test/`.Download and unzip img_highres.zip from the deepfashion inshop dataset at $DATA_ROOT\n",
    "We use DeepFashion dataset and provide our dataset split files, extracted keypoints files and extracted segmentation files for convience.\n",
    "\n",
    "After the processing, you should have the dataset folder formatted like:\n",
    "\n",
    "\n",
    "+ $DATA_ROOT\n",
    "|   + datasets\n",
    "|       + train (all training images)\n",
    "|       |   - xxx.jpg\n",
    "|       |     ...\n",
    "|       +deepfashion\\trainlabels (human parse of all training images)\n",
    "|       |   - xxx.png\n",
    "|       |     ...\n",
    "|       + test (all test images)\n",
    "|       |   - xxx.jpg\n",
    "|       |     ...\n",
    "|       + deepfashion\\testlabels (human parse of all test images)\n",
    "|       |   - xxx.png\n",
    "|       |     ...\n",
    "|       - fashion-resize-pairs-train.csv (paired poses for training)\n",
    "|       - fashion-resize-pairs-test.csv (paired poses for test)\n",
    "|       - fasion-resize-annotation-train.csv (keypoints for training images)\n",
    "|       - fasion-resize-annotation-test.csv  (keypoints for test images)\n",
    "|       - train.lst\n",
    "|       - test.lst\n",
    "|       - standard_test_anns.txt\n",
    "\n",
    "1.Person images\n",
    "Download person images from deep fasion dataset in-shop clothes retrival benchmark and download dataset split from Google Drive.Crop the images. Split the raw images into the train split (`./datasets/train/`) and the test split (`./datasets /test/`.).      \n",
    "\n",
    "2.Keypoints files\n",
    "Download train/test pairs and train/test key points annotations from Google Drive, including fashion-resize-pairs-train.csv,  fashion-resize-pairs-test.csv, fashion-resize-annotation-train.csv, fashion-resize-annotation-train.csv. Put these four files under the (./dataset/) directory.\n",
    "\n",
    "3.Run python tools/generate_fashion_dataset.py --dataroot $DATAROOT to split the data.\n",
    "\n",
    "4.Get human parsing. You can obtain the parsing by either:\n",
    "Run off-the-shelf human parser  on`./ $DATA_ROOT/deepfashion/train/` and `./$DATA_ROOT/deepfashion/test. Name the output parses folder  as`./ $DATA_ROOT/deepfashion/train/trainlabels/` and`./ $DATA_ROOT/deepfashion/test/testlabels/` respectively`./ $DATA_ROOT.Download standard_test_anns.txt for fast visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f61799",
   "metadata": {},
   "source": [
    "## Generating Images Using Pretrained Models\n",
    "\n",
    "Once the dataset is prepared, the reconstruction results be got using pretrained models.\n",
    "\n",
    "\n",
    "1.     Keypoints files\n",
    "\n",
    "    Download train/test pairs and train/test key points annotations from Google Drive, including fashion-resize-pairs-train.csv,           fashion-resize-pairs-test.csv, fashion-resize-annotation-train.csv, fashion-resize-annotation-train.csv. Put these four files         under the deepfashion directory.\n",
    "    \n",
    "\n",
    "2. Generate the reconstruction results using the pretrained model.\n",
    "\t```bash\n",
    "   python test.py --name deepfashion_pretrained --load_size 256 --crop_size 256 --dataset_mode custom --label_dir datasets/test/labels --image_dir datasets/test/images --label_nc 20 --no_instance --gpu_ids 0\n",
    "    ```\n",
    "\n",
    "3. The reconstruction images are saved at `./results/deepfashion_pretrained/` and the corresponding style codes are stored at `./styles_test/style_codes/`.\n",
    "\n",
    "4. Pre-calculate the mean style codes for the UI mode. The mean style codes can be found at `./styles_test/mean_style_code/`.\n",
    "\n",
    "\t```bash\n",
    "    python calculate_mean_style_code.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a0188",
   "metadata": {},
   "source": [
    "## Training New Models\n",
    "\n",
    "To train the new model, you need to specify the option `--dataset_mode custom`, along with `--label_dir [path_to_labels] --image_dir [path_to_images]`. You also need to specify options such as `--label_nc` for the number of label classes in the dataset, and `--no_instance` to denote the dataset doesn't have instance maps.\n",
    "\n",
    "\n",
    "```bash\n",
    "python train.py --name [experiment_name] --load_size 256 --crop_size 256 --dataset_mode custom --label_dir datasets/deepfashion/train/labels --image_dir datasets/deepfashion/train/images --label_nc 20 --no_instance --batchSize 32 --gpu_ids 0,1,2,3\n",
    "```\n",
    "\n",
    "If you only have single GPU with small memory, please use `--batchSize 2 --gpu_ids 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f530bc7d",
   "metadata": {},
   "source": [
    "## UI Introduction\n",
    "\n",
    "We provide a convenient UI for the users to do some extension works. To run the UI mode, you need to:\n",
    "\n",
    "1. run the step **Generating Images Using Pretrained Models** to save the style codes of the test images and the mean style codes. Or you can directly download the style codes from [here](https://drive.google.com/drive/folders/1GRz5HxNX4MKDkScb5s0JjkQHKjfc1Ztx). (Note: if you directly use the downloaded style codes, you have to use the pretrained model.\n",
    "\n",
    "2. Put the visualization images of the labels used for generating in `./imgs/colormaps/` and the style images in `./imgs/style_imgs_test/`. Some example images are provided in these 2 folders. Note: the visualization image and the style image should be picked from `./datasets/deepfashion/test/vis/` and `./datasets/deepfashion/test/labels/`, because only the style codes of the test images are saved in `./styles_test/style_codes/`. If you want to use your own images, please prepare the images, labels and visualization of the labels in `./datasets/deepfashion/test/` with the same format, and calculate the corresponding style codes.\n",
    "\n",
    "3. Run the UI mode\n",
    "\n",
    "    ```bash\n",
    "    python run_UI.py --name deepfashion_pretrained --load_size 256 --crop_size 256 --dataset_mode custom --label_dir datasets/deepfashion/test/labels --image_dir datasets/deepfashion/test/images --label_nc 20 --no_instance --gpu_ids 0\n",
    "    ```\n",
    "4. How to use the UI. Please check the detail usage of the UI from our .\n",
    "\n",
    "\t[![image](./docs/assets/UI.GIF)](https://youtu.be/0Vbj9xFgoUw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d9ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de854f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
